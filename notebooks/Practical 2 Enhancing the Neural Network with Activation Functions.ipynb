{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235d5c4c",
   "metadata": {},
   "source": [
    "# 🌻 Practical 2: Flower Classification with Activation Functions\n",
    "This is **Practical 2** of the **Hands-On Computer Vision Series: A Practical Learning Journey**.\n",
    "\n",
    "In this notebook, we enhance our neural network model by adding **activation functions**, specifically the **ReLU** activation function. This allows the model to learn non-linear patterns and improves its classification capability.\n",
    "\n",
    "We also explain **loss functions**, **learning rate**, and important **hyperparameters** you can tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca95e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f527a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and class names\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "CLASS_NAMES = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c35dde",
   "metadata": {},
   "source": [
    "## ☁️ Using Google Cloud-hosted Flower Dataset\n",
    "This notebook uses flower images and labels stored in **CSV format** on Google Cloud:\n",
    "\n",
    "- `train_set.csv` for training\n",
    "- `eval_set.csv` for evaluation\n",
    "\n",
    "If you are running locally:\n",
    "- Download the dataset and CSVs\n",
    "- Replace the paths with your local file paths\n",
    "- Ensure image paths in the CSV match your local folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ca505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV and parse image-label pairs\n",
    "def read_and_decode(filename, resize_dims):\n",
    "    img_bytes = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=IMG_CHANNELS)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, resize_dims)\n",
    "    return img\n",
    "\n",
    "def parse_csvline(csv_line):\n",
    "    record_default = [\"\", \"\"]\n",
    "    filename, label_string = tf.io.decode_csv(csv_line, record_default)\n",
    "    img = read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    label = tf.argmax(tf.math.equal(CLASS_NAMES, label_string))\n",
    "    return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the batch size here to 16, 32, 64, etc.\n",
    "# Load training and evaluation datasets from Google Cloud\n",
    "train_dataset = (\n",
    "    tf.data.TextLineDataset(\"gs://cloud-ml-data/img/flower_photos/train_set.csv\")\n",
    "    .map(parse_csvline, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(16)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "eval_dataset = (\n",
    "    tf.data.TextLineDataset(\"gs://cloud-ml-data/img/flower_photos/eval_set.csv\")\n",
    "    .map(parse_csvline, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(16)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68758ac5",
   "metadata": {},
   "source": [
    "## 🔧 What is an Activation Function?\n",
    "Activation functions are essential in neural networks because they introduce **non-linearity**. Without them, a model made of multiple layers would behave just like a single-layer linear model.\n",
    "\n",
    "**Why we need them:**\n",
    "- They allow the network to **learn complex patterns** in the data\n",
    "- They help us solve problems that are **not linearly separable**\n",
    "- They add depth to the network’s decision-making ability\n",
    "\n",
    "### 🚀 Common Activation Functions:\n",
    "- 🔸 **ReLU (Rectified Linear Unit):**\n",
    "  - Formula: $f(x) = \\max(0, x)$\n",
    "  - Most widely used due to its simplicity and efficiency\n",
    "  - Helps avoid vanishing gradients\n",
    "\n",
    "- 🔸 **Sigmoid:**\n",
    "  - Formula: $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "  - Squashes values into the range [0, 1]\n",
    "  - Commonly used in binary classification (output layer)\n",
    "\n",
    "- 🔸 **Tanh:**\n",
    "  - Formula: $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "  - Range: [-1, 1], centered at 0\n",
    "  - Can work better than sigmoid for hidden layers\n",
    "\n",
    "- 🔸 **Softmax:**\n",
    "  - Used in the output layer for **multi-class classification**\n",
    "  - Converts raw scores into probabilities that sum to 1\n",
    "\n",
    "➡️ In this notebook, we use **ReLU** for hidden layers and **Softmax** in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298fb511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to adjust the number of hidden units (e.g., 64, 256)\n",
    "# Build the model with activation functions\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer 1\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer 2\n",
    "    tf.keras.layers.Dense(5, activation='softmax')  # Output layer\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c8ee4",
   "metadata": {},
   "source": [
    "## 📊 Understanding Parameters in a Neural Network\n",
    "**Parameters** in a neural network include **weights** and **biases** — the values the model learns during training.\n",
    "\n",
    "**Each Dense (fully connected) layer has:**\n",
    "- 🔹 **Weights**: Connect each input to each output neuron\n",
    "- 🔹 **Biases**: One per output neuron\n",
    "\n",
    "### 🧮 Formula to Calculate Trainable Parameters:\n",
    "`Total Parameters = (input_features × output_units) + output_units`\n",
    "\n",
    "---\n",
    "### 📐 Parameters in This Model:\n",
    "- Input size = `224 × 224 × 3 = 150,528`\n",
    "- Hidden Layer 1 = 128 units\n",
    "- Hidden Layer 2 = 128 units\n",
    "- Output Layer = 5 units\n",
    "\n",
    "**Layer 1 (Input → Hidden 1)**:\n",
    "- Weights: `150,528 × 128 = 19,267,584`\n",
    "- Biases: `128`\n",
    "- Total: `19,267,712`\n",
    "\n",
    "**Layer 2 (Hidden 1 → Hidden 2)**:\n",
    "- Weights: `128 × 128 = 16,384`\n",
    "- Biases: `128`\n",
    "- Total: `16,512`\n",
    "\n",
    "**Layer 3 (Hidden 2 → Output)**:\n",
    "- Weights: `128 × 5 = 640`\n",
    "- Biases: `5`\n",
    "- Total: `645`\n",
    "\n",
    "**Total Parameters = 19,267,712 + 16,512 + 645 = 19,284,869**\n",
    "\n",
    "✅ These are all **trainable** parameters. You can confirm them using the `model.summary()` output.\n",
    "\n",
    "> 📌 More parameters mean more capacity to learn, but also more risk of **overfitting** — which we'll address in the next practical!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648dc06",
   "metadata": {},
   "source": [
    "## 📉 What is a Loss Function?\n",
    "A **loss function** tells the model how far off its predictions are from the actual labels. It is a critical part of training, as it drives how the model learns by adjusting weights to minimize this loss.\n",
    "\n",
    "### 🧪 How it works:\n",
    "1. The model makes a prediction\n",
    "2. The loss function compares this prediction to the true label\n",
    "3. It computes an error (loss)\n",
    "4. This loss is used to update the model’s weights using backpropagation\n",
    "\n",
    "### 🔍 Types of Loss Functions:\n",
    "- 🔹 **SparseCategoricalCrossentropy**:\n",
    "  - Used when labels are integers (not one-hot encoded)\n",
    "  - Perfect for multi-class classification (like our flower dataset)\n",
    "\n",
    "- 🔹 **CategoricalCrossentropy**:\n",
    "  - Use this when your labels are **one-hot encoded** (e.g., [0, 0, 1, 0, 0])\n",
    "\n",
    "- 🔹 **BinaryCrossentropy**:\n",
    "  - Used for binary classification problems (e.g., cat vs. dog)\n",
    "  - Often paired with a sigmoid activation\n",
    "\n",
    "- 🔹 **Mean Squared Error (MSE)**:\n",
    "  - Common in regression tasks\n",
    "\n",
    "➡️ In this practical, we use **SparseCategoricalCrossentropy** because our dataset contains multiple classes and integer labels.\n",
    "The goal during training is to minimize this loss as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe3b67",
   "metadata": {},
   "source": [
    "## 🛠️ Understanding Hyperparameters\n",
    "**Hyperparameters** are external configuration values set before training. They’re not learned from the data, but they directly influence how well your model learns.\n",
    "\n",
    "### 🔑 Common Hyperparameters:\n",
    "- **Learning Rate**: Controls how big each weight update step is.\n",
    "  - Too high → training may diverge\n",
    "  - Too low → training is slow or stuck\n",
    "- **Batch Size**: Number of samples processed before weights are updated.\n",
    "  - Smaller batch size → noisier but more generalized updates\n",
    "  - Larger batch size → faster training but may overfit\n",
    "- **Number of Epochs**: How many times the model sees the entire dataset.\n",
    "  - More epochs → potentially better training (until overfitting begins)\n",
    "- **Number of Hidden Units**: Controls the capacity of the network to learn patterns.\n",
    "  - Too few → underfitting\n",
    "  - Too many → overfitting if regularization isn’t applied\n",
    "- **Image Size**: Affects the resolution of input data and model complexity\n",
    "\n",
    "### 🧪 Try This:\n",
    "- Change `learning_rate=0.001` to `0.0001` or `0.01`\n",
    "- Try different `batch_size` like `16`, `32`, `64`\n",
    "- Increase or decrease the number of neurons in hidden layers\n",
    "- Add more epochs to see performance trends over time\n",
    "\n",
    "🔁 **By experimenting with these values, you’ll gain a deeper intuition for how neural networks learn!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e557a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can experiment with different learning rates here\n",
    "# Try: 0.0001, 0.001 (default), 0.01\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f195cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try increasing the number of epochs to see if accuracy improves or overfits\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=eval_dataset,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a80d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(10)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Acc')\n",
    "plt.plot(epochs_range, val_acc, label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762549f",
   "metadata": {},
   "source": [
    "## ✅ Summary: What We Learned\n",
    "In this practical, you unlocked the power of **non-linear learning** by using activation functions!\n",
    "\n",
    "- 🧠 **ReLU** enabled the model to learn complex, non-linear patterns from image data.\n",
    "- 📈 You saw how to build a deeper model with **two hidden layers**.\n",
    "- 📉 We explained **loss functions** and how models learn by minimizing error.\n",
    "- 🔧 We introduced **hyperparameters** like learning rate and hidden units—feel free to tweak them!\n",
    "\n",
    "🎓 By experimenting with different values, you’ll start to see how model behavior changes in real time.\n",
    "\n",
    "👉 **Up next in Practical 3**:\n",
    "We’ll tackle **overfitting** and how to fight it using techniques like **Dropout**, **Regularization**, and even deeper networks to build more robust models.\n",
    "\n",
    "> The journey continues — keep building, keep tweaking, and most of all, keep learning! 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
